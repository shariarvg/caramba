{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqJHsj_Ckv8K"
      },
      "outputs": [],
      "source": [
        "!pip install -q mamba-ssm causal-conv1d>=1.2.0\n",
        "!pip install -q vllm>=0.5.5\n",
        "!pip install -q accelerate\n",
        "!pip install -q transformers mamba-ssm\n",
        "!pip install -q convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1VN6Hmnu_S9"
      },
      "outputs": [],
      "source": [
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "from convokit import Corpus, download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwuxq_3EN-cF"
      },
      "source": [
        "**MAMBA MENTALITY**\n",
        "\n",
        "Takes into account also the whole idea of continuing to speak or not, switching who's speaking, and ending the sccene/movie. Along with the MLP for actually what would be said."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXCO7jHQNOBp"
      },
      "outputs": [],
      "source": [
        "# Load models\n",
        "model_full = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-370m\").to(\"cuda\")\n",
        "model_character = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-370m\").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "\n",
        "# Combined model with speaker transition and end-of-movie classifier\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, model_full, model_character, embedding_dim=768, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.model_full = model_full\n",
        "        self.model_character = model_character\n",
        "        self.embedding = nn.Embedding(embedding_dim, embedding_dim)\n",
        "\n",
        "        # MLP layers for text prediction\n",
        "        self.fc1 = nn.Linear(embedding_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, tokenizer.vocab_size)  # Output layer for next token prediction\n",
        "\n",
        "        # Turn-taking prediction (binary classification: 0 = continue, 1 = stop)\n",
        "        self.turn_fc = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "        # Speaker transition prediction (categorical classification for speaker_id)\n",
        "        self.speaker_fc = nn.Linear(hidden_dim, 3)  # Assume 3 possible speakers as an example\n",
        "\n",
        "        # End-of-movie prediction (binary classification: 0 = not end, 1 = end)\n",
        "        self.end_fc = nn.Linear(hidden_dim, 2)  # 2 classes: continue or end movie\n",
        "\n",
        "    def forward(self, input_full, input_character):\n",
        "        # Pass through models\n",
        "        full_output = self.model_full(input_full)[\"logits\"]\n",
        "        character_output = self.model_character(input_character)[\"logits\"]\n",
        "\n",
        "        # Apply character mask to char_output (broadcast mask to match vocab dim)\n",
        "        character_mask = character_mask.unsqueeze(-1).expand_as(character_output)\n",
        "        masked_character_output = character_output * character_mask  # zeros out other tokens\n",
        "\n",
        "        # Combine the outputs (concatenate)\n",
        "        combined_output = torch.cat((full_output, masked_character_output), dim=-1)\n",
        "\n",
        "        # Pass through embedding\n",
        "        embedded = self.embedding(combined_output)\n",
        "\n",
        "        # Pass through MLP for token prediction\n",
        "        x = torch.relu(self.fc1(embedded))\n",
        "        token_predictions = self.fc2(x)\n",
        "\n",
        "        # Predict speech turn-taking: whether character stops talking\n",
        "        turn_predictions = self.turn_fc(x)\n",
        "\n",
        "        # Predict speaker transition: who starts speaking next\n",
        "        speaker_predictions = self.speaker_fc(x)\n",
        "\n",
        "        # Predict end of movie: whether the conversation ends\n",
        "        end_predictions = self.end_fc(x)\n",
        "\n",
        "        return token_predictions, turn_predictions, speaker_predictions, end_predictions\n",
        "\n",
        "\n",
        "def combined_loss(token_predictions, turn_predictions, speaker_predictions, end_predictions, target_tokens, target_turns, target_speakers, target_end):\n",
        "    token_loss = nn.CrossEntropyLoss()(token_predictions.view(-1, token_predictions.size(-1)), target_tokens.view(-1))\n",
        "    turn_loss = nn.CrossEntropyLoss()(turn_predictions.view(-1, 2), target_turns.view(-1))\n",
        "    speaker_loss = nn.CrossEntropyLoss()(speaker_predictions.view(-1, speaker_predictions.size(-1)), target_speakers.view(-1))\n",
        "    end_loss = nn.CrossEntropyLoss()(end_predictions.view(-1, 2), target_end.view(-1))\n",
        "\n",
        "    return token_loss + turn_loss + speaker_loss + end_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BmuH29mNwSu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, dialogues, speakers, turns, end, tokenizer, max_length=512):\n",
        "        self.dialogues = dialogues\n",
        "        self.speakers = speakers\n",
        "        self.turns = turns\n",
        "        self.end = end\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dialogue = self.dialogues[idx]\n",
        "        speaker = self.speakers[idx]\n",
        "        turn = self.turns[idx]\n",
        "        end = self.end[idx]\n",
        "\n",
        "        # Tokenize the dialogue\n",
        "        inputs = self.tokenizer(dialogue, return_tensors=\"pt\", truncation=True, padding=True, max_length=self.max_length)\n",
        "\n",
        "        # Convert tokenized data to tensors\n",
        "        input_ids = inputs['input_ids'].squeeze(0)  # Remove the batch dimension\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)  # Same size as input_ids\n",
        "\n",
        "        # Generate token-level speaker IDs based on original speaker list\n",
        "        # For simplicity, assume each dialogue has a single speaker for the entire token sequence\n",
        "        # If needed, this can be refined by tokenizing per sentence and then mapping speaker labels.\n",
        "        speakers_per_token = [self.speakers[idx]] * input_ids.size(0)  # Each token has the same speaker as the dialogue\n",
        "\n",
        "        # Convert speakers list to tensor\n",
        "        token_speaker_ids = torch.tensor(speakers_per_token)\n",
        "\n",
        "        # Create character mask for the given character (assuming you're interested in a specific speaker)\n",
        "        character_mask = (token_speaker_ids == speaker).long()  # Mask for the target character speaking\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'speaker': speaker,\n",
        "            'turn': turn,\n",
        "            'end': end,\n",
        "            'character_mask': character_mask  # Add the character mask to the batch\n",
        "        }\n",
        "\n",
        "# Example dataset and dataloader\n",
        "dialogues = [\"Hello, how are you?\", \"I'm good, thanks! How about you?\"]\n",
        "speakers = [0, 1]  # Assume character 0 speaks first, then character 1\n",
        "turns = [0, 1]  # Speaker 0 continues, then speaker 1 starts\n",
        "end = [0, 1]  # Movie continues after first dialogue, ends after second dialogue\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "dataset = MovieDataset(dialogues, speakers, turns, end, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFZbMCpvNwU8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize model\n",
        "model = CombinedModel(model_full, model_character).to(\"cuda\")\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# To track metrics\n",
        "train_losses, train_ppls = [], []\n",
        "val_losses, val_ppls = [], []\n",
        "\n",
        "epochs = 3  # or more\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_train_token_loss = 0\n",
        "    total_train_ppl = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        # Move to GPU\n",
        "        input_ids = batch['input_ids'].to(\"cuda\")\n",
        "        speaker_labels = batch['speaker'].to(\"cuda\")\n",
        "        turn_labels = batch['turn'].to(\"cuda\")\n",
        "        end_labels = batch['end'].to(\"cuda\")\n",
        "\n",
        "        # Generate character mask\n",
        "        character_mask = batch['speaker'].unsqueeze(-1).to(\"cuda\")  # Shape (batch_size, 1)\n",
        "\n",
        "        # Forward pass\n",
        "        token_preds, turn_preds, speaker_preds, end_preds = model(input_ids, input_ids, character_mask)\n",
        "\n",
        "        # Losses\n",
        "        token_loss = criterion(token_preds.view(-1, token_preds.size(-1)), input_ids.view(-1))\n",
        "        turn_loss = criterion(turn_preds.view(-1, 2), turn_labels.view(-1))\n",
        "\n",
        "        turn_mask = (turn_labels == 1)\n",
        "        if turn_mask.sum() > 0:\n",
        "            speaker_loss = criterion(\n",
        "                speaker_preds[turn_mask].view(-1, speaker_preds.size(-1)),\n",
        "                speaker_labels[turn_mask].view(-1)\n",
        "            )\n",
        "        else:\n",
        "            speaker_loss = 0.0\n",
        "\n",
        "        end_loss = criterion(end_preds.view(-1, 2), end_labels.view(-1))\n",
        "\n",
        "        # Total loss\n",
        "        loss = token_loss + turn_loss + speaker_loss + end_loss\n",
        "        total_train_loss += loss.item()\n",
        "        total_train_token_loss += token_loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_train_ppl += torch.exp(token_loss).item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Average train loss and perplexity\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "    avg_train_ppl = total_train_ppl / num_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_ppls.append(avg_train_ppl)\n",
        "\n",
        "    # Validation Perplexity scores\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    total_val_token_loss = 0\n",
        "    total_val_ppl = 0\n",
        "    num_val_batches = len(val_dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(\"cuda\")\n",
        "            speaker_labels = batch['speaker'].to(\"cuda\")\n",
        "            turn_labels = batch['turn'].to(\"cuda\")\n",
        "            end_labels = batch['end'].to(\"cuda\")\n",
        "\n",
        "            # Generate character mask\n",
        "            character_mask = batch['speaker'].unsqueeze(-1).to(\"cuda\")  # Shape (batch_size, 1)\n",
        "\n",
        "            token_preds, turn_preds, speaker_preds, end_preds = model(input_ids, input_ids, character_mask)\n",
        "\n",
        "            token_loss = criterion(token_preds.view(-1, token_preds.size(-1)), input_ids.view(-1))\n",
        "            turn_loss = criterion(turn_preds.view(-1, 2), turn_labels.view(-1))\n",
        "\n",
        "            turn_mask = (turn_labels == 1)\n",
        "            if turn_mask.sum() > 0:\n",
        "                speaker_loss = criterion(\n",
        "                    speaker_preds[turn_mask].view(-1, speaker_preds.size(-1)),\n",
        "                    speaker_labels[turn_mask].view(-1)\n",
        "                )\n",
        "            else:\n",
        "                speaker_loss = 0.0\n",
        "\n",
        "            end_loss = criterion(end_preds.view(-1, 2), end_labels.view(-1))\n",
        "\n",
        "            val_loss = token_loss + turn_loss + speaker_loss + end_loss\n",
        "            total_val_loss += val_loss.item()\n",
        "            total_val_token_loss += token_loss.item()\n",
        "            total_val_ppl += torch.exp(token_loss).item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    avg_val_ppl = total_val_ppl / num_val_batches\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_ppls.append(avg_val_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} | Train PPL: {avg_train_ppl:.2f} || \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | Val PPL: {avg_val_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5avemXyYQQeh"
      },
      "outputs": [],
      "source": [
        "# Plotting our Train/Test Perplexity scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_ppls, label=\"Train Perplexity\")\n",
        "plt.plot(val_ppls, label=\"Val Perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Train vs Validation Perplexity\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
